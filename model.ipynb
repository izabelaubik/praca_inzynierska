{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.30"
      ],
      "metadata": {
        "id": "6xZLPdJhbat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch] accelerate -U"
      ],
      "metadata": {
        "id": "2_TMVpdcaQc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboard"
      ],
      "metadata": {
        "id": "jrzNJKDahCgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision"
      ],
      "metadata": {
        "id": "U5-whfXPhxcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import EarlyStoppingCallback\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "Mz16VlXgLdz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/dataset.csv')"
      ],
      "metadata": {
        "id": "doT06nrULk9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(df['score'].unique(), df['score'].value_counts())\n",
        "plt.xlabel('Review Score')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Review Scores')\n",
        "plt.show()\n",
        "\n",
        "positive_df = df[df['score'] == 1]\n",
        "negative_df = df[df['score'] == 0]\n",
        "\n",
        "print(len(positive_df['score']))\n",
        "print(len(negative_df['score']))\n",
        "\n",
        "min_count = min(len(positive_df), len(negative_df))\n",
        "\n",
        "positive_df = positive_df.sample(n=min_count, random_state=42)\n",
        "negative_df = negative_df.sample(n=min_count, random_state=42)\n",
        "\n",
        "positive_df = positive_df[:40000] #do usunięcia albo do ograniczenia liczby próbek\n",
        "negative_df = negative_df[:40000] #do usunięcia albo do ograniczenia liczby próbek\n",
        "\n",
        "p_train_ds, p_val_and_test_ds = train_test_split(positive_df, test_size=0.2, random_state=42)\n",
        "p_valid_ds, p_test_ds = train_test_split(p_val_and_test_ds, test_size=0.5, random_state=42)\n",
        "\n",
        "n_train_ds, n_val_and_test_ds = train_test_split(negative_df, test_size=0.2, random_state=42)\n",
        "n_valid_ds, n_test_ds = train_test_split(n_val_and_test_ds, test_size=0.5, random_state=42)\n",
        "\n",
        "# Concatenate the balanced samples to create the balanced dataset\n",
        "train_ds = pd.concat([p_train_ds, n_train_ds]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "valid_ds = pd.concat([p_valid_ds, n_valid_ds]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_ds = pd.concat([p_test_ds, n_test_ds]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(len(train_ds['score']))\n",
        "\n",
        "plt.bar(df['score'].unique(), train_ds['score'].value_counts())\n",
        "plt.xlabel('Review Score')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Review Scores after Cleaning')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WnBPrMJmLfkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define pretrained tokenizer and model\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_ds_tokenized = tokenizer(list(train_ds['text']), padding=True, truncation=True, max_length=512)\n",
        "val_ds_tokenized = tokenizer(list(valid_ds['text']), padding=True, truncation=True, max_length=512)\n",
        "test_ds_tokenized = tokenizer(list(test_ds[\"text\"]), padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Create torch dataset\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels.values if labels is not None else None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels is not None:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "train_dataset = Dataset(train_ds_tokenized, train_ds['score'])\n",
        "val_dataset = Dataset(val_ds_tokenized, valid_ds['score'])\n",
        "test_dataset = Dataset(test_ds_tokenized, test_ds['score'])"
      ],
      "metadata": {
        "id": "qpKFHpyjUpL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Addind TensorBoard to monitor training in progress\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/bert_model/logs"
      ],
      "metadata": {
        "id": "dK5bLyQE5pmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Trainer parameters\n",
        "def compute_metrics(predictions):\n",
        "  labels = predictions.label_ids\n",
        "  predictions = predictions.predictions\n",
        "  predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  accuracy = accuracy_score(labels, predictions)\n",
        "  recall = recall_score(labels, predictions)\n",
        "  precision = precision_score(labels, predictions)\n",
        "  f1 = f1_score(labels, predictions)\n",
        "\n",
        "  # return dict(accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
        "  return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
      ],
      "metadata": {
        "id": "YhQMqv_7UyJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "num_training_samples = len(train_dataset)\n",
        "per_device_train_batch_size = 16\n",
        "steps_per_epoch = num_training_samples // per_device_train_batch_size\n",
        "eval_steps = math.ceil(steps_per_epoch // 3)"
      ],
      "metadata": {
        "id": "dX1Kr6v0_QNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_steps)"
      ],
      "metadata": {
        "id": "7c5AqKV4BYTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/bert_model',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    seed=42,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='/content/drive/MyDrive/bert_model/logs',\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50)\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=args,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=val_dataset,\n",
        "  compute_metrics=compute_metrics,\n",
        "  callbacks=[EarlyStoppingCallback(early_stopping_patience=8)])"
      ],
      "metadata": {
        "id": "TEk1TUYAUz2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train pre-trained model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "kpl0EDWE_DRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f\"Test set results: {test_results}\")"
      ],
      "metadata": {
        "id": "hUwCkhOzw_ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = '/content/drive/MyDrive/bert_model/logs'\n",
        "ea = event_accumulator.EventAccumulator(logdir)\n",
        "ea.Reload()"
      ],
      "metadata": {
        "id": "8xMxa3uX0krN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ea.Tags()['scalars'])"
      ],
      "metadata": {
        "id": "7lv7C5PmBHCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = ea.scalars.Items('train/loss')  # Sprawdź, czy ten tag istnieje\n",
        "validation_loss = ea.scalars.Items('eval/loss')"
      ],
      "metadata": {
        "id": "3pjjDdYL0n6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of epochs: {ea.scalars.Items('eval/loss')}\")"
      ],
      "metadata": {
        "id": "K57YDwmqQtRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wykres dla straty\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot([x.step for x in training_loss], [x.value for x in training_loss], label='Training Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3PFtmeIK4aDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wykres dla straty\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot([x.step for x in training_loss], [x.value for x in training_loss], label='Training Loss')\n",
        "plt.plot([x.step for x in validation_loss], [x.value for x in validation_loss], label='Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0hr9prqS1CXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wyodrębnienie danych dla dokładności\n",
        "validation_accuracy = ea.scalars.Items('eval/accuracy')  # Dokładność walidacyjna/ewaluacyjna\n",
        "\n",
        "# Wykres dla dokładności\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot([x.step for x in validation_accuracy], [x.value for x in validation_accuracy], label='Validation Accuracy')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UupDk9v2j_t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of epochs: {trainer.state.epoch}\")\n",
        "#Zmienna trainer.state.epoch zawiera aktualną liczbę zakończonych epok.\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "\n",
        "# Print training arguments\n",
        "print(args)"
      ],
      "metadata": {
        "id": "YmSww3rGxBin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "\n",
        "x_predict = [\"i love this\", \"I hate this\", 'i like this, but i am afraid of it will not suit me', \"cherries are sweet but my fiancee is sweeter\", 'it was a lovely dinner, but what the hell happened to you two?']\n",
        "x_predict_tokenized = tokenizer(x_predict, padding=True, truncation=True, max_length=512)\n",
        "x_dataset = Dataset(x_predict_tokenized)\n",
        "raw_pred, _, _ = Trainer(model).predict(x_dataset)\n",
        "predictions = np.argmax(raw_pred, axis=1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "Chtdl6aN4Cps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_predict = list(test_ds[\"text\"])\n",
        "x_predict_tokenized = tokenizer(x_predict, padding=True, truncation=True, max_length=512)\n",
        "x_dataset = Dataset(x_predict_tokenized)\n",
        "raw_pred, _, _ = Trainer(model).predict(x_dataset)\n",
        "predictions = np.argmax(raw_pred, axis=1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "Sae25p7Q7I3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model\n",
        "best_cm = confusion_matrix(list(test_ds['score']), predictions)\n",
        "best_df_cm = pd.DataFrame(best_cm, index=[\"Negative\", \"Positive\"], columns=[\"Negative\", \"Positive\"])\n",
        "\n",
        "# Plot confusion matrix for the best model\n",
        "hmap_best = sns.heatmap(best_df_cm, annot=True, fmt=\"d\", cmap=\"PuBu\")\n",
        "hmap_best.yaxis.set_ticklabels(hmap_best.yaxis.get_ticklabels(), ha='right')\n",
        "hmap_best.xaxis.set_ticklabels(hmap_best.xaxis.get_ticklabels(), ha='right')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Prediction')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SGl_VQMJ8ISa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #loading saved model to see if it works:\n",
        "\n",
        "# from transformers import BertForSequenceClassification, BertTokenizer\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# # Model name or path to the directory containing the saved model\n",
        "# model_path = '/content/drive/MyDrive/bert_model/checkpoint-500'\n",
        "# model_name = \"bert-base-uncased\"\n",
        "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "# model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "# from transformers import TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "# # Making predictions\n",
        "# class Dataset(Dataset):\n",
        "#     def __init__(self, encodings, labels=None):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels.values if labels is not None else None\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "#         if self.labels is not None:\n",
        "#             item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "#         return item\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "# x_predict = [\"i love this\", \"I hate this\", 'i like this, but i am afraid of it will not suit me', \"cherries are sweet but my fiancee is sweeter\", 'it was a lovely dinner, but what the hell happened to you two?']\n",
        "# x_predict_tokenized = tokenizer(x_predict, padding=True, truncation=True, max_length=512)\n",
        "# x_dataset = Dataset(x_predict_tokenized)\n",
        "# raw_pred, _, _ = Trainer(model).predict(x_dataset)\n",
        "# predictions = np.argmax(raw_pred, axis=1)\n",
        "# print(predictions)"
      ],
      "metadata": {
        "id": "ilIOrMGQBC99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}